<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Knowledge Nexus</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg-deep: #0a0c10;
            --bg: #0d1117;
            --bg-elevated: #161b22;
            --fg: #e6edf3;
            --fg-muted: #7d8590;
            --accent-primary: #00d4aa;
            --accent-secondary: #0ea5e9;
            --accent-tertiary: #f472b6;
            --accent-warning: #fbbf24;
            --border: #30363d;
            --glow-primary: rgba(0, 212, 170, 0.3);
            --glow-secondary: rgba(14, 165, 233, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Space Grotesk', sans-serif;
            background: var(--bg-deep);
            color: var(--fg);
            min-height: 100vh;
            overflow-x: hidden;
        }

        .mono {
            font-family: 'JetBrains Mono', monospace;
        }

        /* Canvas Background */
        #bg-canvas {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 0;
            pointer-events: none;
        }

        /* Main Container */
        .main-container {
            position: relative;
            z-index: 1;
            min-height: 100vh;
        }

        /* Header */
        .header {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            z-index: 100;
            background: linear-gradient(to bottom, var(--bg-deep) 0%, transparent 100%);
            padding: 1.5rem 2rem 3rem;
        }

        .header-content {
            max-width: 1800px;
            margin: 0 auto;
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 2rem;
        }

        .logo {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .logo-icon {
            width: 48px;
            height: 48px;
            background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary));
            border-radius: 12px;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            overflow: hidden;
        }

        .logo-icon::before {
            content: '';
            position: absolute;
            inset: 2px;
            background: var(--bg-deep);
            border-radius: 10px;
        }

        .logo-icon svg {
            position: relative;
            z-index: 1;
        }

        .logo-text {
            font-size: 1.5rem;
            font-weight: 700;
            letter-spacing: -0.02em;
            background: linear-gradient(135deg, var(--fg), var(--accent-primary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .search-container {
            flex: 1;
            max-width: 500px;
            position: relative;
        }

        .search-input {
            width: 100%;
            background: var(--bg-elevated);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 0.875rem 1.25rem 0.875rem 3rem;
            color: var(--fg);
            font-size: 0.95rem;
            font-family: inherit;
            transition: all 0.3s ease;
        }

        .search-input:focus {
            outline: none;
            border-color: var(--accent-primary);
            box-shadow: 0 0 0 3px var(--glow-primary);
        }

        .search-icon {
            position: absolute;
            left: 1rem;
            top: 50%;
            transform: translateY(-50%);
            color: var(--fg-muted);
        }

        .search-shortcut {
            position: absolute;
            right: 1rem;
            top: 50%;
            transform: translateY(-50%);
            background: var(--bg);
            border: 1px solid var(--border);
            border-radius: 6px;
            padding: 0.25rem 0.5rem;
            font-size: 0.75rem;
            color: var(--fg-muted);
        }

        .header-actions {
            display: flex;
            gap: 0.75rem;
        }

        .btn {
            padding: 0.75rem 1.25rem;
            border-radius: 10px;
            font-weight: 500;
            font-size: 0.9rem;
            cursor: pointer;
            transition: all 0.2s ease;
            border: none;
            font-family: inherit;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .btn-ghost {
            background: transparent;
            color: var(--fg-muted);
            border: 1px solid var(--border);
        }

        .btn-ghost:hover {
            background: var(--bg-elevated);
            color: var(--fg);
            border-color: var(--fg-muted);
        }

        .btn-primary {
            background: linear-gradient(135deg, var(--accent-primary), #00b894);
            color: var(--bg-deep);
            font-weight: 600;
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 24px var(--glow-primary);
        }

        /* Sidebar */
        .sidebar {
            position: fixed;
            left: 0;
            top: 0;
            bottom: 0;
            width: 320px;
            background: var(--bg);
            border-right: 1px solid var(--border);
            z-index: 50;
            padding-top: 100px;
            transform: translateX(0);
            transition: transform 0.3s ease;
        }

        .sidebar.collapsed {
            transform: translateX(-320px);
        }

        .sidebar-content {
            height: 100%;
            overflow-y: auto;
            padding: 1.5rem;
        }

        .sidebar-section {
            margin-bottom: 2rem;
        }

        .sidebar-title {
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: var(--fg-muted);
            margin-bottom: 1rem;
            padding-left: 0.5rem;
        }

        .category-list {
            display: flex;
            flex-direction: column;
            gap: 0.25rem;
        }

        .category-item {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            padding: 0.875rem 1rem;
            border-radius: 10px;
            cursor: pointer;
            transition: all 0.2s ease;
            position: relative;
        }

        .category-item:hover {
            background: var(--bg-elevated);
        }

        .category-item.active {
            background: var(--bg-elevated);
            border-left: 3px solid var(--accent-primary);
        }

        .category-icon {
            width: 36px;
            height: 36px;
            border-radius: 8px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.1rem;
        }

        .category-info {
            flex: 1;
        }

        .category-name {
            font-weight: 500;
            font-size: 0.95rem;
            margin-bottom: 0.125rem;
        }

        .category-count {
            font-size: 0.8rem;
            color: var(--fg-muted);
        }

        .category-arrow {
            color: var(--fg-muted);
            transition: transform 0.2s ease;
        }

        .category-item:hover .category-arrow {
            transform: translateX(4px);
        }

        /* Main Content */
        .main-content {
            margin-left: 320px;
            padding: 100px 2rem 2rem;
            transition: margin-left 0.3s ease;
        }

        .main-content.expanded {
            margin-left: 0;
        }

        /* View Toggle */
        .view-toggle {
            display: flex;
            gap: 0.5rem;
            background: var(--bg-elevated);
            padding: 0.375rem;
            border-radius: 10px;
            margin-bottom: 2rem;
        }

        .view-btn {
            padding: 0.625rem 1.25rem;
            border-radius: 8px;
            font-weight: 500;
            font-size: 0.875rem;
            cursor: pointer;
            transition: all 0.2s ease;
            background: transparent;
            color: var(--fg-muted);
            border: none;
            font-family: inherit;
        }

        .view-btn.active {
            background: var(--bg);
            color: var(--fg);
        }

        .view-btn:hover:not(.active) {
            color: var(--fg);
        }

        /* Graph View */
        .graph-container {
            position: relative;
            width: 100%;
            height: calc(100vh - 200px);
            background: var(--bg);
            border-radius: 16px;
            border: 1px solid var(--border);
            overflow: hidden;
        }

        #graph-canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }

        .graph-controls {
            position: absolute;
            bottom: 1.5rem;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 0.5rem;
            background: var(--bg-elevated);
            padding: 0.5rem;
            border-radius: 12px;
            border: 1px solid var(--border);
            z-index: 10;
        }

        .graph-control-btn {
            width: 40px;
            height: 40px;
            border-radius: 8px;
            background: var(--bg);
            border: 1px solid var(--border);
            color: var(--fg);
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s ease;
        }

        .graph-control-btn:hover {
            background: var(--bg-elevated);
            border-color: var(--accent-primary);
        }

        /* Node Details Panel */
        .node-panel {
            position: fixed;
            right: 0;
            top: 0;
            bottom: 0;
            width: 480px;
            background: var(--bg);
            border-left: 1px solid var(--border);
            z-index: 60;
            transform: translateX(100%);
            transition: transform 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            overflow-y: auto;
        }

        .node-panel.open {
            transform: translateX(0);
        }

        .node-panel-header {
            position: sticky;
            top: 0;
            background: linear-gradient(to bottom, var(--bg) 70%, transparent);
            padding: 1.5rem;
            z-index: 10;
        }

        .node-panel-close {
            position: absolute;
            top: 1.5rem;
            right: 1.5rem;
            width: 36px;
            height: 36px;
            border-radius: 8px;
            background: var(--bg-elevated);
            border: 1px solid var(--border);
            color: var(--fg-muted);
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s ease;
        }

        .node-panel-close:hover {
            background: var(--bg);
            color: var(--fg);
            border-color: var(--fg-muted);
        }

        .node-badge {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.375rem 0.75rem;
            background: var(--bg-elevated);
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 500;
            margin-bottom: 1rem;
            border: 1px solid var(--border);
        }

        .node-title {
            font-size: 1.75rem;
            font-weight: 700;
            letter-spacing: -0.02em;
            margin-bottom: 0.5rem;
        }

        .node-subtitle {
            color: var(--fg-muted);
            font-size: 0.95rem;
        }

        .node-panel-body {
            padding: 0 1.5rem 1.5rem;
        }

        .node-section {
            margin-bottom: 2rem;
        }

        .node-section-title {
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: var(--fg-muted);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .node-description {
            font-size: 0.95rem;
            line-height: 1.7;
            color: var(--fg);
        }

        .related-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 0.75rem;
        }

        .related-item {
            padding: 1rem;
            background: var(--bg-elevated);
            border-radius: 10px;
            border: 1px solid var(--border);
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .related-item:hover {
            border-color: var(--accent-primary);
            transform: translateY(-2px);
        }

        .related-item-name {
            font-weight: 500;
            margin-bottom: 0.25rem;
        }

        .related-item-type {
            font-size: 0.8rem;
            color: var(--fg-muted);
        }

        .code-block {
            background: var(--bg-deep);
            border-radius: 10px;
            padding: 1rem;
            overflow-x: auto;
            border: 1px solid var(--border);
        }

        .code-block code {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.85rem;
            color: var(--accent-primary);
        }

        .tag-list {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .tag {
            padding: 0.375rem 0.75rem;
            background: var(--bg-elevated);
            border-radius: 20px;
            font-size: 0.8rem;
            color: var(--fg-muted);
            border: 1px solid var(--border);
        }

        /* Stats Bar */
        .stats-bar {
            display: flex;
            gap: 2rem;
            padding: 1.5rem;
            background: var(--bg-elevated);
            border-radius: 12px;
            margin-bottom: 2rem;
            border: 1px solid var(--border);
        }

        .stat-item {
            text-align: center;
        }

        .stat-value {
            font-size: 2rem;
            font-weight: 700;
            background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .stat-label {
            font-size: 0.85rem;
            color: var(--fg-muted);
            margin-top: 0.25rem;
        }

        /* List View */
        .list-view {
            display: none;
        }

        .list-view.active {
            display: block;
        }

        .graph-view-container {
            display: block;
        }

        .graph-view-container.hidden {
            display: none;
        }

        .term-card {
            background: var(--bg);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 1.5rem;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .term-card:hover {
            border-color: var(--accent-primary);
            transform: translateY(-2px);
        }

        .term-card-header {
            display: flex;
            align-items: flex-start;
            justify-content: space-between;
            margin-bottom: 1rem;
        }

        .term-card-title {
            font-size: 1.25rem;
            font-weight: 600;
        }

        .term-card-category {
            font-size: 0.75rem;
            padding: 0.25rem 0.5rem;
            background: var(--bg-elevated);
            border-radius: 6px;
            color: var(--fg-muted);
        }

        .term-card-desc {
            color: var(--fg-muted);
            font-size: 0.9rem;
            line-height: 1.6;
        }

        .term-card-footer {
            display: flex;
            gap: 0.5rem;
            margin-top: 1rem;
            padding-top: 1rem;
            border-top: 1px solid var(--border);
        }

        /* MiniMap */
        .minimap {
            position: absolute;
            bottom: 1.5rem;
            right: 1.5rem;
            width: 180px;
            height: 120px;
            background: var(--bg-elevated);
            border-radius: 8px;
            border: 1px solid var(--border);
            overflow: hidden;
            z-index: 10;
        }

        #minimap-canvas {
            width: 100%;
            height: 100%;
        }

        /* Animations */
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }

        @keyframes float {
            0%, 100% { transform: translateY(0); }
            50% { transform: translateY(-10px); }
        }

        .fade-in {
            animation: fadeIn 0.5s ease forwards;
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--fg-muted);
        }

        /* Tooltip */
        .tooltip {
            position: absolute;
            background: var(--bg-elevated);
            border: 1px solid var(--border);
            padding: 0.5rem 0.75rem;
            border-radius: 8px;
            font-size: 0.85rem;
            pointer-events: none;
            z-index: 1000;
            opacity: 0;
            transform: translateY(5px);
            transition: all 0.2s ease;
            max-width: 250px;
        }

        .tooltip.visible {
            opacity: 1;
            transform: translateY(0);
        }

        /* Mobile */
        @media (max-width: 768px) {
            .sidebar {
                transform: translateX(-320px);
            }

            .sidebar.open {
                transform: translateX(0);
            }

            .main-content {
                margin-left: 0;
            }

            .node-panel {
                width: 100%;
            }

            .header-content {
                flex-wrap: wrap;
            }

            .search-container {
                order: 3;
                max-width: 100%;
            }

            .stats-bar {
                flex-wrap: wrap;
            }

            .related-grid {
                grid-template-columns: 1fr;
            }
        }

        /* Reduced motion */
        @media (prefers-reduced-motion: reduce) {
            * {
                animation-duration: 0.01ms !important;
                animation-iteration-count: 1 !important;
                transition-duration: 0.01ms !important;
            }
        }

        /* Legend */
        .legend {
            position: absolute;
            top: 1.5rem;
            left: 1.5rem;
            background: var(--bg-elevated);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 1rem;
            z-index: 10;
        }

        .legend-title {
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--fg-muted);
            margin-bottom: 0.75rem;
        }

        .legend-items {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
        }

        .legend-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.85rem;
        }

        .legend-color {
            width: 12px;
            height: 12px;
            border-radius: 50%;
        }

        /* Connection strength indicator */
        .connection-strength {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }

        .strength-bar {
            height: 4px;
            border-radius: 2px;
            background: var(--border);
            flex: 1;
        }

        .strength-fill {
            height: 100%;
            border-radius: 2px;
            transition: width 0.3s ease;
        }
    </style>
</head>
<body>
    <canvas id="bg-canvas"></canvas>

    <div class="main-container">
        <!-- Header -->
        <header class="header">
            <div class="header-content">
                <div class="logo">
                    <div class="logo-icon">
                        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="color: var(--accent-primary)">
                            <circle cx="12" cy="12" r="3"/>
                            <path d="M12 2v4m0 12v4M2 12h4m12 0h4"/>
                            <circle cx="12" cy="12" r="9" stroke-dasharray="4 2"/>
                        </svg>
                    </div>
                    <span class="logo-text">AI Knowledge Nexus</span>
                </div>

                <div class="search-container">
                    <svg class="search-icon" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="11" cy="11" r="8"/>
                        <path d="m21 21-4.35-4.35"/>
                    </svg>
                    <input type="text" class="search-input" placeholder="Search AI concepts, terms, relationships..." id="search-input" aria-label="Search AI concepts">
                    <span class="search-shortcut mono">/</span>
                </div>

                <div class="header-actions">
                    <button class="btn btn-ghost" id="sidebar-toggle" aria-label="Toggle sidebar">
                        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M3 12h18M3 6h18M3 18h18"/>
                        </svg>
                    </button>
                    <button class="btn btn-primary" id="add-term-btn">
                        <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M12 5v14m-7-7h14"/>
                        </svg>
                        Add Term
                    </button>
                </div>
            </div>
        </header>

        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-content">
                <div class="sidebar-section">
                    <h3 class="sidebar-title">Categories</h3>
                    <div class="category-list" id="category-list">
                        <!-- Dynamically populated -->
                    </div>
                </div>

                <div class="sidebar-section">
                    <h3 class="sidebar-title">Quick Filters</h3>
                    <div class="category-list">
                        <div class="category-item" data-filter="all">
                            <div class="category-icon" style="background: rgba(0, 212, 170, 0.15); color: var(--accent-primary);">
                                <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="10"/>
                                </svg>
                            </div>
                            <div class="category-info">
                                <div class="category-name">All Terms</div>
                                <div class="category-count" id="total-count">0 terms</div>
                            </div>
                        </div>
                        <div class="category-item" data-filter="fundamental">
                            <div class="category-icon" style="background: rgba(251, 191, 36, 0.15); color: var(--accent-warning);">
                                <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <polygon points="12 2 15.09 8.26 22 9.27 17 14.14 18.18 21.02 12 17.77 5.82 21.02 7 14.14 2 9.27 8.91 8.26 12 2"/>
                                </svg>
                            </div>
                            <div class="category-info">
                                <div class="category-name">Fundamental</div>
                                <div class="category-count">Core concepts</div>
                            </div>
                        </div>
                        <div class="category-item" data-filter="recent">
                            <div class="category-icon" style="background: rgba(14, 165, 233, 0.15); color: var(--accent-secondary);">
                                <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                    <circle cx="12" cy="12" r="10"/>
                                    <polyline points="12 6 12 12 16 14"/>
                                </svg>
                            </div>
                            <div class="category-info">
                                <div class="category-name">Recently Added</div>
                                <div class="category-count">New terms</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content" id="main-content">
            <div class="view-toggle">
                <button class="view-btn active" data-view="graph">Graph View</button>
                <button class="view-btn" data-view="list">List View</button>
            </div>

            <div class="stats-bar">
                <div class="stat-item">
                    <div class="stat-value" id="stat-categories">0</div>
                    <div class="stat-label">Categories</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value" id="stat-terms">0</div>
                    <div class="stat-label">Total Terms</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value" id="stat-connections">0</div>
                    <div class="stat-label">Connections</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value" id="stat-depth">0</div>
                    <div class="stat-label">Max Depth</div>
                </div>
            </div>

            <!-- Graph View -->
            <div class="graph-view-container" id="graph-view">
                <div class="graph-container">
                    <canvas id="graph-canvas"></canvas>
                    
                    <div class="legend">
                        <div class="legend-title">Node Types</div>
                        <div class="legend-items">
                            <div class="legend-item">
                                <div class="legend-color" style="background: var(--accent-primary);"></div>
                                <span>Core Concept</span>
                            </div>
                            <div class="legend-item">
                                <div class="legend-color" style="background: var(--accent-secondary);"></div>
                                <span>Technique</span>
                            </div>
                            <div class="legend-item">
                                <div class="legend-color" style="background: var(--accent-tertiary);"></div>
                                <span>Infrastructure</span>
                            </div>
                            <div class="legend-item">
                                <div class="legend-color" style="background: var(--accent-warning);"></div>
                                <span>Application</span>
                            </div>
                        </div>
                    </div>

                    <div class="graph-controls">
                        <button class="graph-control-btn" id="zoom-in" aria-label="Zoom in">
                            <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <circle cx="11" cy="11" r="8"/><path d="m21 21-4.35-4.35M11 8v6m-3-3h6"/>
                            </svg>
                        </button>
                        <button class="graph-control-btn" id="zoom-out" aria-label="Zoom out">
                            <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <circle cx="11" cy="11" r="8"/><path d="m21 21-4.35-4.35M8 11h6"/>
                            </svg>
                        </button>
                        <button class="graph-control-btn" id="reset-view" aria-label="Reset view">
                            <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <path d="M3 12a9 9 0 1 0 9-9 9.75 9.75 0 0 0-6.74 2.74L3 8"/>
                                <path d="M3 3v5h5"/>
                            </svg>
                        </button>
                        <button class="graph-control-btn" id="fullscreen" aria-label="Fullscreen">
                            <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <path d="M8 3H5a2 2 0 0 0-2 2v3m18 0V5a2 2 0 0 0-2-2h-3m0 18h3a2 2 0 0 0 2-2v-3M3 16v3a2 2 0 0 0 2 2h3"/>
                            </svg>
                        </button>
                    </div>

                    <div class="minimap">
                        <canvas id="minimap-canvas"></canvas>
                    </div>
                </div>
            </div>

            <!-- List View -->
            <div class="list-view" id="list-view">
                <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4" id="terms-list">
                    <!-- Dynamically populated -->
                </div>
            </div>
        </main>

        <!-- Node Details Panel -->
        <aside class="node-panel" id="node-panel">
            <div class="node-panel-header">
                <button class="node-panel-close" id="close-panel" aria-label="Close panel">
                    <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M18 6 6 18M6 6l12 12"/>
                    </svg>
                </button>
                <div class="node-badge" id="node-badge">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="10"/>
                    </svg>
                    <span id="node-category">Category</span>
                </div>
                <h2 class="node-title" id="node-title">Term Name</h2>
                <p class="node-subtitle" id="node-subtitle">Short description</p>
            </div>
            <div class="node-panel-body">
                <div class="node-section">
                    <h3 class="node-section-title">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"/>
                            <polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/>
                        </svg>
                        Definition
                    </h3>
                    <p class="node-description" id="node-description">Full description goes here...</p>
                </div>

                <div class="node-section">
                    <h3 class="node-section-title">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"/>
                            <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"/>
                        </svg>
                        Related Terms
                    </h3>
                    <div class="related-grid" id="related-terms">
                        <!-- Dynamically populated -->
                    </div>
                </div>

                <div class="node-section">
                    <h3 class="node-section-title">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <polyline points="16 18 22 12 16 6"/><polyline points="8 6 2 12 8 18"/>
                        </svg>
                        Code Example
                    </h3>
                    <div class="code-block" id="code-example">
                        <code>// No code example available</code>
                    </div>
                </div>

                <div class="node-section">
                    <h3 class="node-section-title">
                        <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"/>
                            <line x1="7" y1="7" x2="7.01" y2="7"/>
                        </svg>
                        Tags
                    </h3>
                    <div class="tag-list" id="tag-list">
                        <!-- Dynamically populated -->
                    </div>
                </div>
            </div>
        </aside>

        <!-- Add Term Modal -->
        <div class="fixed inset-0 bg-black/70 z-[200] hidden items-center justify-center" id="add-term-modal">
            <div class="bg-[var(--bg)] border border-[var(--border)] rounded-2xl w-full max-w-lg mx-4 max-h-[90vh] overflow-y-auto">
                <div class="p-6 border-b border-[var(--border)]">
                    <h2 class="text-xl font-bold">Add New Term</h2>
                    <p class="text-[var(--fg-muted)] text-sm mt-1">Contribute to the AI Knowledge Nexus</p>
                </div>
                <form class="p-6 space-y-4" id="add-term-form">
                    <div>
                        <label class="block text-sm font-medium mb-2">Term Name *</label>
                        <input type="text" name="name" required class="w-full bg-[var(--bg-elevated)] border border-[var(--border)] rounded-lg px-4 py-3 text-[var(--fg)] focus:outline-none focus:border-[var(--accent-primary)]">
                    </div>
                    <div>
                        <label class="block text-sm font-medium mb-2">Category *</label>
                        <select name="category" required class="w-full bg-[var(--bg-elevated)] border border-[var(--border)] rounded-lg px-4 py-3 text-[var(--fg)] focus:outline-none focus:border-[var(--accent-primary)]">
                            <!-- Dynamically populated -->
                        </select>
                    </div>
                    <div>
                        <label class="block text-sm font-medium mb-2">Short Description *</label>
                        <input type="text" name="shortDesc" required class="w-full bg-[var(--bg-elevated)] border border-[var(--border)] rounded-lg px-4 py-3 text-[var(--fg)] focus:outline-none focus:border-[var(--accent-primary)]">
                    </div>
                    <div>
                        <label class="block text-sm font-medium mb-2">Full Definition *</label>
                        <textarea name="definition" rows="4" required class="w-full bg-[var(--bg-elevated)] border border-[var(--border)] rounded-lg px-4 py-3 text-[var(--fg)] focus:outline-none focus:border-[var(--accent-primary)]"></textarea>
                    </div>
                    <div>
                        <label class="block text-sm font-medium mb-2">Related Terms (comma-separated)</label>
                        <input type="text" name="related" class="w-full bg-[var(--bg-elevated)] border border-[var(--border)] rounded-lg px-4 py-3 text-[var(--fg)] focus:outline-none focus:border-[var(--accent-primary)]" placeholder="LLMs, Transformers, Attention">
                    </div>
                    <div>
                        <label class="block text-sm font-medium mb-2">Tags (comma-separated)</label>
                        <input type="text" name="tags" class="w-full bg-[var(--bg-elevated)] border border-[var(--border)] rounded-lg px-4 py-3 text-[var(--fg)] focus:outline-none focus:border-[var(--accent-primary)]" placeholder="core, architecture, fundamentals">
                    </div>
                    <div class="flex gap-3 pt-4">
                        <button type="button" class="btn btn-ghost flex-1" id="cancel-add-term">Cancel</button>
                        <button type="submit" class="btn btn-primary flex-1">Add Term</button>
                    </div>
                </form>
            </div>
        </div>
    </div>

    <div class="tooltip" id="tooltip"></div>

    <script>
        // ==========================================
        // AI KNOWLEDGE DATA STRUCTURE (Extensible)
        // ==========================================
        const AIKnowledgeBase = {
            categories: [
                {
                    id: 'rag',
                    name: 'RAG',
                    fullName: 'Retrieval Augmented Generation',
                    icon: 'database',
                    color: '#00d4aa',
                    description: 'Techniques for augmenting LLMs with external knowledge'
                },
                {
                    id: 'agentic',
                    name: 'Agentic AI',
                    fullName: 'Agentic Artificial Intelligence',
                    icon: 'bot',
                    color: '#0ea5e9',
                    description: 'Autonomous AI systems that can plan and execute tasks'
                },
                {
                    id: 'mcp',
                    name: 'MCP',
                    fullName: 'Model Context Protocol',
                    icon: 'protocol',
                    color: '#f472b6',
                    description: 'Protocol for connecting AI assistants to systems'
                },
                {
                    id: 'architecture',
                    name: 'Architecture',
                    fullName: 'Model Architecture',
                    icon: 'layers',
                    color: '#fbbf24',
                    description: 'Fundamental neural network architectures'
                },
                {
                    id: 'training',
                    name: 'Training',
                    fullName: 'Training & Fine-tuning',
                    icon: 'activity',
                    color: '#a78bfa',
                    description: 'Methods for training and adapting models'
                },
                {
                    id: 'prompting',
                    name: 'Prompting',
                    fullName: 'Prompt Engineering',
                    icon: 'terminal',
                    color: '#34d399',
                    description: 'Techniques for effective model interaction'
                },
                {
                    id: 'infrastructure',
                    name: 'Infrastructure',
                    fullName: 'AI Infrastructure',
                    icon: 'server',
                    color: '#f87171',
                    description: 'Systems and tools for AI deployment'
                },
                {
                    id: 'applications',
                    name: 'Applications',
                    fullName: 'AI Applications',
                    icon: 'app',
                    color: '#60a5fa',
                    description: 'Real-world AI implementations'
                }
            ],
            
            terms: [
                // RAG Section
                {
                    id: 'rag',
                    name: 'RAG',
                    fullName: 'Retrieval Augmented Generation',
                    category: 'rag',
                    type: 'core',
                    shortDesc: 'Augmenting LLM responses with retrieved external knowledge',
                    definition: 'RAG is a technique that combines the generative capabilities of large language models with external knowledge retrieval. Instead of relying solely on parametric knowledge stored in model weights, RAG systems retrieve relevant documents from a knowledge base and use them to ground responses in factual, up-to-date information. This reduces hallucinations and enables access to information beyond the training cutoff.',
                    related: ['vector-database', 'embeddings', 'chunking', 'reranking', 'semantic-search'],
                    tags: ['core', 'production', 'knowledge-retrieval'],
                    codeExample: `# Basic RAG Pipeline
from langchain.retrievers import VectorStoreRetriever
from langchain.vectorstores import Chroma

# 1. Index documents
vectorstore = Chroma.from_documents(
    documents=chunked_docs,
    embedding=OpenAIEmbeddings()
)

# 2. Retrieve relevant context
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

# 3. Generate with context
docs = retriever.get_relevant_documents(query)
response = llm.generate(
    prompt=f"Context: {docs}\\nQuestion: {query}"
)`
                },
                {
                    id: 'vector-database',
                    name: 'Vector Database',
                    category: 'rag',
                    type: 'infrastructure',
                    shortDesc: 'Specialized databases for storing and querying vector embeddings',
                    definition: 'Vector databases are specialized storage systems designed for efficient similarity search over high-dimensional vector embeddings. They use approximate nearest neighbor (ANN) algorithms like HNSW, IVF, or LSH to enable fast retrieval even at scale. Popular options include Pinecone, Weaviate, Chroma, Milvus, and pgvector.',
                    related: ['embeddings', 'rag', 'semantic-search', 'hnsw'],
                    tags: ['infrastructure', 'storage', 'similarity-search'],
                    codeExample: `# Vector Database Operations
import pinecone

# Create index
pinecone.create_index(
    "knowledge-base",
    dimension=1536,
    metric="cosine"
)

# Upsert vectors
index.upsert([
    ("doc1", embedding1, {"text": "...", "source": "..."}),
    ("doc2", embedding2, {"text": "...", "source": "..."})
])

# Query
results = index.query(
    vector=query_embedding,
    top_k=10,
    include_metadata=True
)`
                },
                {
                    id: 'embeddings',
                    name: 'Embeddings',
                    category: 'rag',
                    type: 'technique',
                    shortDesc: 'Dense vector representations of text or data',
                    definition: 'Embeddings are dense vector representations that capture semantic meaning in a continuous space. Text embeddings map words, sentences, or documents to vectors where semantically similar items are close in the vector space. Modern embedding models like OpenAI\'s text-embedding-3, Cohere embeddings, or open-source options like BERT and Sentence Transformers enable semantic search, clustering, and retrieval.',
                    related: ['vector-database', 'semantic-search', 'rag', 'transformers'],
                    tags: ['fundamental', 'representations', 'semantic'],
                    codeExample: `# Generate Embeddings
from openai import OpenAI
client = OpenAI()

response = client.embeddings.create(
    model="text-embedding-3-small",
    input="Your text here"
)

embedding = response.data[0].embedding
# Returns 1536-dimensional vector

# Similarity calculation
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

similarity = cosine_similarity(embedding1, embedding2)`
                },
                {
                    id: 'chunking',
                    name: 'Chunking',
                    category: 'rag',
                    type: 'technique',
                    shortDesc: 'Splitting documents into manageable pieces for retrieval',
                    definition: 'Chunking is the process of splitting documents into smaller, semantically meaningful pieces for embedding and retrieval. Good chunking strategies maintain context while keeping chunks focused. Methods include fixed-size chunking, recursive character splitting, semantic chunking based on meaning boundaries, and parent-document retrieval that indexes small chunks but retrieves larger context.',
                    related: ['rag', 'embeddings', 'document-loader'],
                    tags: ['preprocessing', 'retrieval', 'documents'],
                    codeExample: `# Chunking Strategies
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    SemanticChunker
)

# Recursive splitting
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\\n\\n", "\\n", ". ", " "]
)
chunks = splitter.split_text(document)

# Semantic chunking
semantic_splitter = SemanticChunker(
    embeddings_model,
    breakpoint_threshold_type="percentile"
)
semantic_chunks = semantic_splitter.split_text(document)`
                },
                {
                    id: 'reranking',
                    name: 'Reranking',
                    category: 'rag',
                    type: 'technique',
                    shortDesc: 'Re-scoring retrieved documents for better relevance',
                    definition: 'Reranking is a two-stage retrieval technique where an initial fast retrieval (using vector similarity) is followed by a more sophisticated cross-encoder model that scores document-query pairs. This significantly improves precision at the cost of latency. Models like Cohere Rerank, BGE Reranker, or ColBERT provide much better relevance scoring than pure vector similarity.',
                    related: ['rag', 'semantic-search', 'retrieval'],
                    tags: ['retrieval', 'optimization', 'precision'],
                    codeExample: `# Reranking Pipeline
from cohere import Client

co = Client("api_key")

# Initial retrieval
initial_docs = vectorstore.similarity_search(query, k=20)

# Rerank
results = co.rerank(
    query=query,
    documents=[doc.page_content for doc in initial_docs],
    top_n=5,
    model="rerank-english-v3.0"
)

# Get top reranked docs
top_docs = [initial_docs[r.index] for r in results.results]`
                },
                {
                    id: 'semantic-search',
                    name: 'Semantic Search',
                    category: 'rag',
                    type: 'technique',
                    shortDesc: 'Finding documents by meaning rather than keywords',
                    definition: 'Semantic search uses embeddings to find documents based on meaning rather than exact keyword matches. It enables finding relevant content even when users use different vocabulary than the source material. Combined with keyword search in hybrid approaches, it provides robust retrieval that handles both vocabulary mismatches and precise term matching.',
                    related: ['embeddings', 'vector-database', 'rag', 'hybrid-search'],
                    tags: ['search', 'retrieval', 'semantic'],
                    codeExample: `# Semantic Search Implementation
def semantic_search(query, vectorstore, k=5):
    # Encode query
    query_embedding = embedding_model.encode(query)
    
    # Search
    results = vectorstore.similarity_search_by_vector(
        query_embedding,
        k=k
    )
    return results

# Hybrid Search (Semantic + Keyword)
def hybrid_search(query, vectorstore, bm25_index, alpha=0.5):
    semantic_results = vectorstore.similarity_search(query, k=20)
    keyword_results = bm25_index.search(query, k=20)
    
    # Reciprocal Rank Fusion
    return rrf_fusion(semantic_results, keyword_results, alpha)`
                },

                // Agentic Section
                {
                    id: 'agentic-ai',
                    name: 'Agentic AI',
                    category: 'agentic',
                    type: 'core',
                    shortDesc: 'AI systems that autonomously plan and execute tasks',
                    definition: 'Agentic AI refers to AI systems capable of autonomous goal-directed behavior. Unlike single-prompt interactions, agentic systems can plan multi-step actions, use external tools, maintain memory across interactions, and self-correct when things go wrong. Key frameworks include LangChain Agents, AutoGPT, CrewAI, and the ReAct (Reasoning + Acting) paradigm.',
                    related: ['tool-use', 'planning', 'memory', 'multi-agent', 'react-prompting'],
                    tags: ['core', 'autonomous', 'framework'],
                    codeExample: `# Agentic Loop (ReAct Pattern)
def agent_loop(query, max_iterations=10):
    for i in range(max_iterations):
        # Reason
        thought = llm.generate(
            f"Question: {query}\\nThought: {history}"
        )
        
        # Decide action
        action = parse_action(thought)
        
        if action.type == "FINISH":
            return action.answer
        
        # Execute
        observation = execute_tool(action.tool, action.input)
        history.append(f"Thought: {thought}\\nAction: {action}\\nObservation: {observation}")`
                },
                {
                    id: 'tool-use',
                    name: 'Tool Use',
                    category: 'agentic',
                    type: 'technique',
                    shortDesc: 'Enabling LLMs to interact with external systems',
                    definition: 'Tool use (or function calling) allows LLMs to interact with external systems by generating structured outputs that trigger predefined functions. The model doesn\'t execute code directly - it outputs parameters in a specified format, which are then executed by the runtime environment. This enables searching the web, querying databases, calling APIs, and any other programmatic action.',
                    related: ['agentic-ai', 'function-calling', 'mcp', 'planning'],
                    tags: ['agent', 'integration', 'external'],
                    codeExample: `# Tool Definition & Use
tools = [
    {
        "type": "function",
        "function": {
            "name": "search_web",
            "description": "Search the web for current information",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"}
                },
                "required": ["query"]
            }
        }
    }
]

# Model generates tool call
response = client.chat.completions.create(
    model="gpt-4-turbo",
    messages=[...],
    tools=tools
)

# Execute tool and return result
tool_call = response.choices[0].message.tool_calls[0]
result = search_web(tool_call.function.arguments["query"])`
                },
                {
                    id: 'planning',
                    name: 'Planning',
                    category: 'agentic',
                    type: 'technique',
                    shortDesc: 'Breaking complex goals into executable steps',
                    definition: 'Planning in agentic AI involves decomposing complex goals into a sequence of actionable steps. Techniques include chain-of-thought prompting for implicit planning, explicit plan generation followed by execution, and dynamic replanning when execution fails. Advanced approaches like Tree of Thoughts explore multiple plan branches before committing.',
                    related: ['agentic-ai', 'tool-use', 'reasoning', 'chain-of-thought'],
                    tags: ['agent', 'reasoning', 'decomposition'],
                    codeExample: `# Planning Agent
def plan_and_execute(goal):
    # Generate plan
    plan = llm.generate(f"""
    Goal: {goal}
    Create a step-by-step plan. Format:
    1. [action] description
    2. [action] description
    """)
    
    steps = parse_plan(plan)
    results = []
    
    for step in steps:
        result = execute_step(step)
        results.append(result)
        
        # Replan if needed
        if result.failed:
            new_plan = replan(goal, steps, result)
            steps = update_plan(steps, new_plan)
    
    return synthesize_results(results)`
                },
                {
                    id: 'memory',
                    name: 'Agent Memory',
                    category: 'agentic',
                    type: 'technique',
                    shortDesc: 'Persisting context across agent interactions',
                    definition: 'Agent memory systems enable AI to retain information across interactions. Types include: short-term memory (conversation history within a session), long-term memory (persistent storage of facts and experiences), episodic memory (past interaction sequences), and working memory (current task context). Vector databases often power semantic memory retrieval.',
                    related: ['agentic-ai', 'vector-database', 'context-window', 'rag'],
                    tags: ['agent', 'persistence', 'context'],
                    codeExample: `# Memory Systems
class AgentMemory:
    def __init__(self):
        self.short_term = []  # Recent messages
        self.long_term = VectorStore()  # Semantic memory
        self.working = {}  # Current task state
    
    def add_message(self, message):
        self.short_term.append(message)
        # Summarize and store important info
        if len(self.short_term) > 20:
            summary = summarize(self.short_term[:10])
            self.long_term.add(summary)
            self.short_term = self.short_term[10:]
    
    def recall(self, query):
        # Retrieve relevant memories
        return self.long_term.similarity_search(query, k=5)`
                },
                {
                    id: 'multi-agent',
                    name: 'Multi-Agent Systems',
                    category: 'agentic',
                    type: 'technique',
                    shortDesc: 'Coordinating multiple AI agents for complex tasks',
                    definition: 'Multi-agent systems coordinate multiple specialized AI agents to solve complex problems. Agents can have different roles (researcher, writer, reviewer), share a common memory, and communicate through structured messages. Frameworks like CrewAI, AutoGen, and LangGraph enable orchestrating agent teams with defined workflows and handoffs.',
                    related: ['agentic-ai', 'planning', 'tool-use', 'communication'],
                    tags: ['agent', 'coordination', 'teams'],
                    codeExample: `# Multi-Agent System (CrewAI style)
from crewai import Agent, Task, Crew

researcher = Agent(
    role="Researcher",
    goal="Find comprehensive information",
    tools=[search_tool, web_tool],
    backstory="Expert at finding and synthesizing information"
)

writer = Agent(
    role="Writer",
    goal="Create engaging content",
    tools=[write_tool],
    backstory="Skilled at turning research into clear prose"
)

crew = Crew(
    agents=[researcher, writer],
    tasks=[
        Task(description="Research topic X", agent=researcher),
        Task(description="Write article", agent=writer)
    ]
)

result = crew.kickoff()`
                },

                // MCP Section
                {
                    id: 'mcp',
                    name: 'MCP',
                    fullName: 'Model Context Protocol',
                    category: 'mcp',
                    type: 'core',
                    shortDesc: 'Open protocol for connecting AI to data sources and tools',
                    definition: 'The Model Context Protocol (MCP) is an open standard that enables AI assistants to connect to external systems uniformly. It provides a standardized way to expose resources (files, data), prompts (reusable templates), and tools (functions) to AI models. MCP servers can be built for any data source or tool, and any MCP-compatible client can use them.',
                    related: ['tool-use', 'function-calling', 'resources', 'prompts'],
                    tags: ['protocol', 'standard', 'integration'],
                    codeExample: `# MCP Server Implementation
from mcp.server import Server
from mcp.types import Tool, Resource

server = Server("my-server")

@server.tool()
def search_database(query: str) -> str:
    """Search internal database"""
    return db.search(query)

@server.resource("file://documents/{path}")
def get_document(path: str) -> str:
    """Access document files"""
    return open(f"documents/{path}").read()

# Client usage
client.connect_to_server("my-server")
tools = client.list_tools()
result = client.call_tool("search_database", {"query": "AI"})`
                },
                {
                    id: 'mcp-resources',
                    name: 'MCP Resources',
                    category: 'mcp',
                    type: 'technique',
                    shortDesc: 'Exposing data through MCP with URI-based access',
                    definition: 'MCP Resources provide a URI-based way to expose data to AI models. Resources can represent files, database records, API responses, or any other data. They support templates for dynamic URIs, subscriptions for real-time updates, and can be listed or read directly. Resources are read-only - use tools for mutations.',
                    related: ['mcp', 'mcp-tools', 'prompts'],
                    tags: ['mcp', 'data', 'read-only'],
                    codeExample: `# Resource Definition
@server.resource("database://users/{user_id}")
def get_user(user_id: str) -> dict:
    return db.users.find(user_id)

@server.resource("api://weather/{location}")
def get_weather(location: str) -> dict:
    return weather_api.get(location)

# List all resources
resources = client.list_resources()
# [Resource(uri="database://users/123", name="User 123"), ...]

# Read specific resource
data = client.read_resource("database://users/456")`
                },
                {
                    id: 'mcp-tools',
                    name: 'MCP Tools',
                    category: 'mcp',
                    type: 'technique',
                    shortDesc: 'Functions exposed through MCP for AI execution',
                    definition: 'MCP Tools are functions that AI models can call to perform actions. Unlike resources (read-only), tools can modify state, make API calls, or execute any operation. Tools include JSON Schema for parameters, enabling type-safe invocation. The model decides when to call tools based on user intent and tool descriptions.',
                    related: ['mcp', 'mcp-resources', 'tool-use', 'function-calling'],
                    tags: ['mcp', 'functions', 'actions'],
                    codeExample: `# Tool Definition with Schema
@server.tool()
def send_email(
    to: str,
    subject: str,
    body: str
) -> dict:
    """Send an email to a recipient.
    
    Args:
        to: Recipient email address
        subject: Email subject line
        body: Email body text
    
    Returns:
        Confirmation with message ID
    """
    return email_service.send(to, subject, body)

# Tools include schemas for validation
tool_schema = {
    "name": "send_email",
    "inputSchema": {
        "type": "object",
        "properties": {
            "to": {"type": "string"},
            "subject": {"type": "string"},
            "body": {"type": "string"}
        },
        "required": ["to", "subject", "body"]
    }
}`
                },

                // Architecture Section
                {
                    id: 'transformers',
                    name: 'Transformers',
                    category: 'architecture',
                    type: 'core',
                    shortDesc: 'The foundational architecture behind modern LLMs',
                    definition: 'Transformers are neural network architectures that use self-attention mechanisms to process sequences in parallel, rather than sequentially like RNNs. Introduced in "Attention is All You Need" (2017), they enable capturing long-range dependencies efficiently. All major LLMs (GPT, Claude, Llama) are transformer-based, using either encoder-only (BERT), decoder-only (GPT), or encoder-decoder (T5) variants.',
                    related: ['attention-mechanism', 'llm', 'positional-encoding', 'self-attention'],
                    tags: ['fundamental', 'architecture', 'deep-learning'],
                    codeExample: `# Simplified Transformer Block
import torch.nn as nn

class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, n_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
    
    def forward(self, x):
        # Self-attention with residual
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        
        # Feed-forward with residual
        ff_out = self.ff(x)
        x = self.norm2(x + ff_out)
        return x`
                },
                {
                    id: 'attention-mechanism',
                    name: 'Attention Mechanism',
                    category: 'architecture',
                    type: 'core',
                    shortDesc: 'Allowing models to focus on relevant input parts',
                    definition: 'Attention mechanisms allow neural networks to dynamically focus on relevant parts of the input when producing each output. Self-attention computes relationships between all positions in a sequence. Multi-head attention runs multiple attention operations in parallel, capturing different types of relationships. Scaled dot-product attention is the standard implementation.',
                    related: ['transformers', 'self-attention', 'multi-head-attention'],
                    tags: ['fundamental', 'architecture', 'mechanism'],
                    codeExample: `# Scaled Dot-Product Attention
import torch
import torch.nn.functional as F

def attention(query, key, value, mask=None):
    d_k = query.size(-1)
    
    # Compute attention scores
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    
    # Apply mask (for padding or causal attention)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # Softmax over last dimension
    attn_weights = F.softmax(scores, dim=-1)
    
    # Weighted sum of values
    output = torch.matmul(attn_weights, value)
    return output, attn_weights`
                },
                {
                    id: 'llm',
                    name: 'LLM',
                    fullName: 'Large Language Model',
                    category: 'architecture',
                    type: 'core',
                    shortDesc: 'Foundation models trained on massive text corpora',
                    definition: 'Large Language Models are neural networks trained on vast text corpora to predict the next token. Through this simple objective, they learn language understanding, world knowledge, reasoning, and generation capabilities. Modern LLMs like GPT-4, Claude, Llama, and Mistral range from 7B to trillions of parameters and can be adapted via fine-tuning or prompting for diverse tasks.',
                    related: ['transformers', 'tokenization', 'pre-training', 'emergent-abilities'],
                    tags: ['fundamental', 'foundation-model', 'generation'],
                    codeExample: `# LLM Text Generation
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b")

def generate(prompt, max_new_tokens=100):
    inputs = tokenizer(prompt, return_tensors="pt")
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)`
                },
                {
                    id: 'tokenization',
                    name: 'Tokenization',
                    category: 'architecture',
                    type: 'technique',
                    shortDesc: 'Converting text to model-processable tokens',
                    definition: 'Tokenization converts raw text into tokens that models process. Modern LLMs use subword tokenization (BPE, WordPiece, SentencePiece) that balances vocabulary size with sequence length. Tokens are then mapped to embedding vectors. Different tokenizers produce different token counts for the same text, affecting cost and context usage.',
                    related: ['llm', 'embeddings', 'context-window'],
                    tags: ['preprocessing', 'fundamentals', 'encoding'],
                    codeExample: `# Tokenization with tiktoken
import tiktoken

# Different models use different tokenizers
enc = tiktoken.encoding_for_model("gpt-4")

text = "Hello, world!"
tokens = enc.encode(text)  # [9906, 11, 1917, 0]
decoded = enc.decode(tokens)  # "Hello, world!"

# Count tokens for API usage
def count_tokens(text, model="gpt-4"):
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))

# Note: Different tokenizers produce different results
print(f"GPT-4 tokens: {count_tokens(text, 'gpt-4')}")
print(f"GPT-3.5 tokens: {count_tokens(text, 'gpt-3.5-turbo')}")`
                },
                {
                    id: 'context-window',
                    name: 'Context Window',
                    category: 'architecture',
                    type: 'technique',
                    shortDesc: 'Maximum sequence length a model can process',
                    definition: 'The context window is the maximum number of tokens a model can process in a single forward pass. This includes both input and output tokens. Larger context windows enable processing longer documents but increase computational cost quadratically for standard attention. Techniques like RoPE scaling, ALiBi, and sparse attention help extend effective context.',
                    related: ['llm', 'tokenization', 'attention-mechanism', 'kv-cache'],
                    tags: ['limitations', 'architecture', 'performance'],
                    codeExample: `# Context Window Management
class ContextManager:
    def __init__(self, max_tokens=4096):
        self.max_tokens = max_tokens
        self.messages = []
    
    def add_message(self, role, content, tokenizer):
        new_tokens = len(tokenizer.encode(content))
        current_tokens = sum(len(tokenizer.encode(m["content"])) for m in self.messages)
        
        # Truncate oldest messages if needed
        while current_tokens + new_tokens > self.max_tokens - 500:  # Reserve for response
            self.messages.pop(0)
            current_tokens = sum(len(tokenizer.encode(m["content"])) for m in self.messages)
        
        self.messages.append({"role": role, "content": content})`
                },

                // Training Section
                {
                    id: 'pre-training',
                    name: 'Pre-training',
                    category: 'training',
                    type: 'core',
                    shortDesc: 'Initial training on massive datasets for general capabilities',
                    definition: 'Pre-training is the foundational training phase where models learn from massive unlabeled text corpora. The objective is typically next-token prediction, teaching the model language patterns, world knowledge, and reasoning abilities. Pre-training requires enormous compute (thousands of GPUs for months) and produces the base model that can then be fine-tuned.',
                    related: ['llm', 'fine-tuning', 'next-token-prediction', 'scaling-laws'],
                    tags: ['training', 'foundation', 'compute'],
                    codeExample: `# Simplified Pre-training Loop
def pretrain(model, dataloader, optimizer, steps):
    model.train()
    
    for step, batch in enumerate(dataloader):
        # Next token prediction
        input_ids = batch["input_ids"]
        labels = batch["labels"]  # Shifted input_ids
        
        # Forward pass
        outputs = model(input_ids, labels=labels)
        loss = outputs.loss
        
        # Backward pass
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        if step % 100 == 0:
            print(f"Step {step}, Loss: {loss.item():.4f}")
        
        if step >= steps:
            break`
                },
                {
                    id: 'fine-tuning',
                    name: 'Fine-tuning',
                    category: 'training',
                    type: 'technique',
                    shortDesc: 'Adapting pre-trained models for specific tasks',
                    definition: 'Fine-tuning adapts a pre-trained model to specific tasks or domains by continuing training on labeled data. Methods include full fine-tuning (updating all weights), parameter-efficient fine-tuning (PEFT) like LoRA that only updates small adapter modules, and instruction tuning that teaches the model to follow instructions. Fine-tuning is much cheaper than pre-training.',
                    related: ['pre-training', 'lora', 'instruction-tuning', 'rlhf'],
                    tags: ['training', 'adaptation', 'specialization'],
                    codeExample: `# Fine-tuning with LoRA
from peft import LoraConfig, get_peft_model

# Load base model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")

# LoRA configuration
lora_config = LoraConfig(
    r=16,  # Low rank
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 4M || all params: 7B || trainable%: 0.06%

# Fine-tune on task data
trainer.train()`
                },
                {
                    id: 'rlhf',
                    name: 'RLHF',
                    fullName: 'Reinforcement Learning from Human Feedback',
                    category: 'training',
                    type: 'technique',
                    shortDesc: 'Aligning models with human preferences',
                    definition: 'RLHF aligns language models with human preferences by training a reward model on human comparisons, then optimizing the LLM to maximize this reward. The process: (1) collect human rankings of model outputs, (2) train a reward model to predict preferences, (3) use PPO to optimize the LLM policy. This makes models more helpful, harmless, and honest.',
                    related: ['fine-tuning', 'dpo', 'alignment', 'reward-model'],
                    tags: ['alignment', 'training', 'human-feedback'],
                    codeExample: `# RLHF Pipeline Overview
# Step 1: Train Reward Model
def train_reward_model(model, comparison_data):
    for prompt, chosen, rejected in comparison_data:
        chosen_score = model(prompt, chosen)
        rejected_score = model(prompt, rejected)
        
        # Bradley-Terry loss
        loss = -torch.log(
            torch.sigmoid(chosen_score - rejected_score)
        )
        loss.backward()

# Step 2: PPO Optimization
def ppo_step(policy, reward_model, prompt):
    response = policy.generate(prompt)
    reward = reward_model(prompt, response)
    
    # PPO clipped objective
    ratio = new_prob / old_prob
    clipped = torch.clamp(ratio, 0.8, 1.2)
    loss = -torch.min(ratio * reward, clipped * reward)
    loss.backward()`
                },
                {
                    id: 'dpo',
                    name: 'DPO',
                    fullName: 'Direct Preference Optimization',
                    category: 'training',
                    type: 'technique',
                    shortDesc: 'Simpler alternative to RLHF for alignment',
                    definition: 'DPO (Direct Preference Optimization) is a simpler alternative to RLHF that directly optimizes the policy using preference data without training a separate reward model. It reparameterizes the reward function in terms of the policy, enabling direct optimization with a simple classification loss on preferred vs rejected responses.',
                    related: ['rlhf', 'fine-tuning', 'alignment'],
                    tags: ['alignment', 'training', 'simplified'],
                    codeExample: `# DPO Loss Implementation
def dpo_loss(policy, ref_policy, prompt, chosen, rejected, beta=0.1):
    """
    Direct Preference Optimization loss
    
    beta: controls strength of KL constraint
    """
    # Get log probabilities
    chosen_logp = policy.log_prob(prompt, chosen)
    rejected_logp = policy.log_prob(prompt, rejected)
    
    # Reference model log probs (frozen)
    with torch.no_grad():
        ref_chosen_logp = ref_policy.log_prob(prompt, chosen)
        ref_rejected_logp = ref_policy.log_prob(prompt, rejected)
    
    # DPO objective
    chosen_reward = beta * (chosen_logp - ref_chosen_logp)
    rejected_reward = beta * (rejected_logp - ref_rejected_logp)
    
    loss = -torch.log(
        torch.sigmoid(chosen_reward - rejected_reward)
    ).mean()
    
    return loss`
                },
                {
                    id: 'lora',
                    name: 'LoRA',
                    fullName: 'Low-Rank Adaptation',
                    category: 'training',
                    type: 'technique',
                    shortDesc: 'Parameter-efficient fine-tuning with low-rank matrices',
                    definition: 'LoRA (Low-Rank Adaptation) enables efficient fine-tuning by adding small trainable rank-decomposition matrices to existing weights. Instead of updating the full weight matrix W, LoRA learns two smaller matrices A and B such that W' = W + BA. This reduces trainable parameters by 10,000x while maintaining comparable performance to full fine-tuning.',
                    related: ['fine-tuning', 'qlora', 'peft'],
                    tags: ['efficient', 'fine-tuning', 'adaptation'],
                    codeExample: `# LoRA Implementation
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=16, alpha=32):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        
        # Original weight (frozen)
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.weight.requires_grad = False
        
        # LoRA matrices (trainable)
        self.lora_A = nn.Parameter(torch.randn(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
    def forward(self, x):
        # W'x = Wx + (B @ A) @ x * scaling
        result = x @ self.weight.T
        result += (x @ self.lora_A.T @ self.lora_B.T) * self.scaling
        return result`
                },

                // Prompting Section
                {
                    id: 'prompt-engineering',
                    name: 'Prompt Engineering',
                    category: 'prompting',
                    type: 'core',
                    shortDesc: 'The art of crafting effective LLM inputs',
                    definition: 'Prompt engineering is the practice of designing inputs to elicit desired outputs from language models. Techniques include clear instruction specification, providing examples (few-shot), breaking down complex tasks (chain-of-thought), and iterative refinement. Good prompting dramatically improves model performance without any training.',
                    related: ['chain-of-thought', 'few-shot', 'system-prompt', 'prompt-templates'],
                    tags: ['fundamental', 'techniques', 'optimization'],
                    codeExample: `# Prompt Engineering Best Practices

# 1. Be Specific and Clear
vague_prompt = "Write about AI"
better_prompt = """
Write a 500-word blog post about the impact of AI on healthcare.
Target audience: Healthcare professionals.
Tone: Professional but accessible.
Include: 3 specific examples of AI applications in hospitals.
Structure: Introduction, 3 sections, conclusion.
"""

# 2. Provide Examples (Few-shot)
few_shot_prompt = """
Convert the following to SQL:

Example 1:
Question: Find all users over 25
SQL: SELECT * FROM users WHERE age > 25;

Example 2:
Question: Count orders per customer
SQL: SELECT customer_id, COUNT(*) FROM orders GROUP BY customer_id;

Question: Find average order value by month
SQL:
"""

# 3. Chain of Thought
cot_prompt = """
Solve this step by step:
1. Identify the key information
2. Determine the relationships
3. Calculate the answer
4. Verify your result

Problem: {problem}
"""`
                },
                {
                    id: 'chain-of-thought',
                    name: 'Chain of Thought',
                    fullName: 'Chain-of-Thought Prompting',
                    category: 'prompting',
                    type: 'technique',
                    shortDesc: 'Eliciting reasoning through step-by-step decomposition',
                    definition: 'Chain-of-Thought (CoT) prompting improves reasoning by asking models to think step-by-step before giving a final answer. This simple technique dramatically improves performance on math, logic, and complex reasoning tasks. Zero-shot CoT adds "Let\'s think step by step" while few-shot CoT provides worked examples with reasoning.',
                    related: ['prompt-engineering', 'reasoning', 'tree-of-thoughts', 'planning'],
                    tags: ['reasoning', 'technique', 'prompting'],
                    codeExample: `# Chain-of-Thought Prompting

# Zero-shot CoT
zero_shot_cot = """
Question: A store has 23 apples. They sell 15 and receive a new shipment of 8. How many apples now?

Let's think step by step.
"""

# Few-shot CoT
few_shot_cot = """
Question: Roger has 5 tennis balls. He buys 2 cans of 3 balls each. How many balls?
Answer: Roger started with 5 balls. 2 cans of 3 balls each is 6 balls. 5 + 6 = 11.

Question: The cafeteria had 23 apples. If they used 20 for lunch and bought 6 more, how many?
Answer: The cafeteria started with 23 apples. They used 20, leaving 3. Then bought 6 more. 3 + 6 = 9.

Question: A store has 23 apples. They sell 15 and receive a new shipment of 8. How many?
Answer:
"""`
                },
                {
                    id: 'few-shot',
                    name: 'Few-Shot Prompting',
                    category: 'prompting',
                    type: 'technique',
                    shortDesc: 'Providing examples to guide model behavior',
                    definition: 'Few-shot prompting provides the model with examples of input-output pairs before the actual query. This helps the model understand the desired format, style, and reasoning pattern without any weight updates. Research shows 2-6 examples often work best, and example selection and ordering significantly impact performance.',
                    related: ['prompt-engineering', 'in-context-learning', 'prompt-templates'],
                    tags: ['technique', 'examples', 'in-context'],
                    codeExample: `# Few-Shot Prompting Patterns

# Classification
classification_prompt = """
Classify the sentiment:

Text: "This product exceeded my expectations!"
Sentiment: Positive

Text: "Worst purchase I've ever made."
Sentiment: Negative

Text: "It's okay, nothing special."
Sentiment: Neutral

Text: "Absolutely love it! Best thing ever!"
Sentiment:
"""

# Format Following
format_prompt = """
Extract entities:

Text: "John Smith works at Google in New York."
Entities: {"person": "John Smith", "company": "Google", "location": "New York"}

Text: "Sarah Johnson is CEO of Tesla, based in Austin."
Entities: {"person": "Sarah Johnson", "company": "Tesla", "location": "Austin"}

Text: "Elon Musk announced SpaceX's new launch from Florida."
Entities:
"""`
                },
                {
                    id: 'system-prompt',
                    name: 'System Prompt',
                    category: 'prompting',
                    type: 'technique',
                    shortDesc: 'Defining model behavior and constraints',
                    definition: 'System prompts are instructions that define the model\'s behavior, personality, capabilities, and constraints. They\'re processed before user messages and persist throughout a conversation. Good system prompts specify the role, tone, output format, what to do and avoid, and any safety guidelines. They\'re crucial for consistent, controlled AI behavior.',
                    related: ['prompt-engineering', 'prompt-templates', 'prompt-injection'],
                    tags: ['configuration', 'behavior', 'instructions'],
                    codeExample: `# System Prompt Examples

# Role Definition
system_prompt_assistant = """
You are a helpful AI assistant specializing in Python programming.
- Provide clear, well-commented code examples
- Explain concepts at an intermediate level
- Suggest best practices and common pitfalls
- If unsure, acknowledge limitations
"""

# Constraint Definition
system_prompt_structured = """
You are a data analyst assistant.
ALWAYS respond in this JSON format:
{
  "analysis": "brief summary",
  "insights": ["insight1", "insight2"],
  "recommendations": ["rec1", "rec2"],
  "confidence": "high|medium|low"
}
Do not include any text outside the JSON structure.
"""

# API Usage
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": system_prompt_assistant},
        {"role": "user", "content": user_query}
    ]
)`
                },

                // Infrastructure Section
                {
                    id: 'inference',
                    name: 'Inference',
                    category: 'infrastructure',
                    type: 'core',
                    shortDesc: 'Running trained models for predictions',
                    definition: 'Inference is the process of running a trained model to generate outputs. Unlike training, inference doesn\'t update weights. Key considerations include latency (time per request), throughput (requests per second), cost, and accuracy. Optimization techniques include quantization, batching, caching, and specialized hardware (GPUs, TPUs, LPUs).',
                    related: ['quantization', 'kv-cache', 'model-serving', 'latency'],
                    tags: ['deployment', 'production', 'optimization'],
                    codeExample: `# Inference Optimization

# 1. Batching
def batch_inference(model, inputs, batch_size=32):
    results = []
    for i in range(0, len(inputs), batch_size):
        batch = inputs[i:i+batch_size]
        outputs = model.generate(batch)
        results.extend(outputs)
    return results

# 2. Continuous Batching (for serving)
class ContinuousBatcher:
    def __init__(self, model, max_batch_size=32):
        self.model = model
        self.queue = []
        
    async def generate(self, prompt):
        self.queue.append(prompt)
        if len(self.queue) >= self.max_batch_size:
            batch = self.queue[:]
            self.queue.clear()
            return await self.model.generate_batch(batch)

# 3. KV Cache usage
def generate_with_cache(model, prompt, max_tokens):
    past_key_values = None
    generated = []
    
    for _ in range(max_tokens):
        output, past_key_values = model(
            prompt if not generated else next_token,
            past_key_values=past_key_values,
            use_cache=True
        )
        next_token = sample(output)
        generated.append(next_token)`
                },
                {
                    id: 'quantization',
                    name: 'Quantization',
                    category: 'infrastructure',
                    type: 'technique',
                    shortDesc: 'Reducing model precision for efficiency',
                    definition: 'Quantization reduces the memory and compute requirements of models by lowering numerical precision (e.g., FP16 to INT8 or INT4). This can be done post-training (PTQ) or during training (QAT). Modern quantization methods like GPTQ, AWQ, and GGUF maintain most model quality while reducing size by 2-4x and enabling faster inference.',
                    related: ['inference', 'model-compression', 'qlora'],
                    tags: ['optimization', 'efficiency', 'deployment'],
                    codeExample: `# Quantization Methods

# 1. PyTorch Native Quantization
import torch.quantization as quant

model = load_model()
model.eval()

# Dynamic quantization
quantized_model = quant.quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)

# 2. bitsandbytes for LLMs
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-70b",
    quantization_config=bnb_config
)

# 3. GGUF for CPU inference
# Use llama.cpp for efficient CPU/GPU hybrid inference`
                },
                {
                    id: 'kv-cache',
                    name: 'KV Cache',
                    fullName: 'Key-Value Cache',
                    category: 'infrastructure',
                    type: 'technique',
                    shortDesc: 'Caching attention states for faster generation',
                    definition: 'The KV Cache stores computed key and value tensors during autoregressive generation. Instead of recomputing attention for all previous tokens at each step, the model reuses cached values and only computes for the new token. This reduces generation from O(n) to O(n) complexity, dramatically speeding up inference at the cost of memory.',
                    related: ['inference', 'attention-mechanism', 'context-window'],
                    tags: ['optimization', 'memory', 'generation'],
                    codeExample: `# KV Cache Implementation

class KVCache:
    def __init__(self, n_layers, n_heads, head_dim, max_seq_len):
        self.cache = [
            torch.zeros(1, n_heads, max_seq_len, head_dim)
            for _ in range(2 * n_layers)  # K and V for each layer
        ]
        self.seq_len = 0
    
    def update(self, layer_idx, new_k, new_v):
        # Append new keys and values
        k_cache, v_cache = self.cache[2*layer_idx], self.cache[2*layer_idx+1]
        k_cache[:, :, self.seq_len:self.seq_len+1] = new_k
        v_cache[:, :, self.seq_len:self.seq_len+1] = new_v
        return k_cache[:, :, :self.seq_len+1], v_cache[:, :, :self.seq_len+1]
    
    def increment(self):
        self.seq_len += 1

# Usage in attention
def attention_with_cache(query, cache, layer_idx):
    k, v = cache.update(layer_idx, new_k, new_v)
    output = scaled_dot_product_attention(query, k, v)
    cache.increment()
    return output`
                },

                // Applications Section
                {
                    id: 'chatbots',
                    name: 'Chatbots',
                    category: 'applications',
                    type: 'application',
                    shortDesc: 'Conversational AI interfaces',
                    definition: 'Chatbots are conversational AI systems that interact with users through natural language dialogue. Modern chatbots use LLMs for natural conversation combined with RAG for knowledge, tools for actions, and memory for context. They range from simple Q&A systems to sophisticated agents that can complete complex tasks across multiple turns.',
                    related: ['rag', 'agentic-ai', 'system-prompt', 'memory'],
                    tags: ['application', 'conversation', 'interface'],
                    codeExample: `# Chatbot Architecture
class Chatbot:
    def __init__(self, llm, rag_system, tools, memory):
        self.llm = llm
        self.rag = rag_system
        self.tools = tools
        self.memory = memory
    
    async def respond(self, user_input):
        # 1. Store user message
        self.memory.add_message("user", user_input)
        
        # 2. Retrieve relevant context
        context = await self.rag.retrieve(user_input)
        
        # 3. Check for tool use
        if self.should_use_tool(user_input):
            tool_result = await self.tools.execute(user_input)
            context += f"\\nTool Result: {tool_result}"
        
        # 4. Generate response
        messages = self.build_messages(user_input, context)
        response = await self.llm.generate(messages)
        
        # 5. Store response
        self.memory.add_message("assistant", response)
        return response`
                },
                {
                    id: 'code-generation',
                    name: 'Code Generation',
                    category: 'applications',
                    type: 'application',
                    shortDesc: 'AI-powered code writing and assistance',
                    definition: 'Code generation uses LLMs trained on code to write, complete, explain, and debug software. Models like GitHub Copilot, CodeLlama, and StarCoder can generate code from natural language descriptions, complete partial code, write tests, and translate between languages. They integrate into IDEs for real-time assistance.',
                    related: ['llm', 'fine-tuning', 'tool-use'],
                    tags: ['application', 'development', 'productivity'],
                    codeExample: `# Code Generation System
class CodeAssistant:
    def __init__(self, model, language="python"):
        self.model = model
        self.language = language
    
    def generate(self, prompt, context=""):
        full_prompt = f"""
        Language: {self.language}
        Context: {context}
        
        Task: {prompt}
        
        Generate clean, well-commented code.
        Include error handling and docstrings.
        """
        return self.model.generate(full_prompt)
    
    def complete(self, code_prefix):
        return self.model.generate(
            code_prefix,
            stop_sequences=["\\n\\n", "def ", "class "]
        )
    
    def explain(self, code):
        prompt = f"Explain this code:\\n```\\n{code}\\n```"
        return self.model.generate(prompt)
    
    def test(self, code):
        prompt = f"Write unit tests for:\\n```\\n{code}\\n```"
        return self.model.generate(prompt)`
                }
            ]
        };

        // ==========================================
        // STATE MANAGEMENT
        // ==========================================
        const state = {
            currentView: 'graph',
            selectedCategory: null,
            selectedNode: null,
            searchQuery: '',
            graph: {
                zoom: 1,
                panX: 0,
                panY: 0,
                nodes: [],
                edges: []
            },
            hoveredNode: null,
            isDragging: false,
            lastMousePos: { x: 0, y: 0 }
        };

        // ==========================================
        // INITIALIZE NODE POSITIONS
        // ==========================================
        function initializeGraphNodes() {
            const centerX = window.innerWidth / 2 - 160;
            const centerY = window.innerHeight / 2;
            
            // Group terms by category
            const categories = {};
            AIKnowledgeBase.terms.forEach(term => {
                if (!categories[term.category]) {
                    categories[term.category] = [];
                }
                categories[term.category].push(term);
            });
            
            const categoryAngles = {};
            const categoryCount = Object.keys(categories).length;
            let angleOffset = 0;
            
            Object.keys(categories).forEach((catId, index) => {
                categoryAngles[catId] = (index / categoryCount) * Math.PI * 2 - Math.PI / 2;
            });
            
            // Position nodes in category clusters
            AIKnowledgeBase.terms.forEach((term, index) => {
                const categoryAngle = categoryAngles[term.category] || 0;
                const nodesInCategory = categories[term.category].length;
                const nodeIndex = categories[term.category].indexOf(term);
                
                // Determine radius based on type
                let baseRadius = 300;
                if (term.type === 'core') baseRadius = 200;
                else if (term.type === 'technique') baseRadius = 280;
                else if (term.type === 'infrastructure') baseRadius = 340;
                else baseRadius = 380;
                
                // Spread within category
                const spreadAngle = Math.PI / 6;
                const nodeAngle = categoryAngle + (nodeIndex - nodesInCategory / 2) * (spreadAngle / nodesInCategory);
                
                // Add some randomness
                const radiusJitter = (Math.random() - 0.5) * 50;
                const angleJitter = (Math.random() - 0.5) * 0.2;
                
                state.graph.nodes.push({
                    id: term.id,
                    x: centerX + Math.cos(nodeAngle + angleJitter) * (baseRadius + radiusJitter),
                    y: centerY + Math.sin(nodeAngle + angleJitter) * (baseRadius + radiusJitter),
                    vx: 0,
                    vy: 0,
                    radius: term.type === 'core' ? 35 : term.type === 'technique' ? 28 : 22,
                    term: term
                });
            });
            
            // Create edges from relationships
            AIKnowledgeBase.terms.forEach(term => {
                if (term.related) {
                    term.related.forEach(relatedId => {
                        const sourceNode = state.graph.nodes.find(n => n.id === term.id);
                        const targetNode = state.graph.nodes.find(n => n.id === relatedId);
                        if (sourceNode && targetNode) {
                            // Avoid duplicate edges
                            const exists = state.graph.edges.some(e => 
                                (e.source === term.id && e.target === relatedId) ||
                                (e.source === relatedId && e.target === term.id)
                            );
                            if (!exists) {
                                state.graph.edges.push({
                                    source: term.id,
                                    target: relatedId,
                                    strength: 0.5
                                });
                            }
                        }
                    });
                }
            });
        }

        // ==========================================
        // CANVAS BACKGROUND ANIMATION
        // ==========================================
        const bgCanvas = document.getElementById('bg-canvas');
        const bgCtx = bgCanvas.getContext('2d');
        let bgParticles = [];

        function initBgCanvas() {
            bgCanvas.width = window.innerWidth;
            bgCanvas.height = window.innerHeight;
            
            bgParticles = [];
            const particleCount = Math.min(100, Math.floor((bgCanvas.width * bgCanvas.height) / 15000));
            
            for (let i = 0; i < particleCount; i++) {
                bgParticles.push({
                    x: Math.random() * bgCanvas.width,
                    y: Math.random() * bgCanvas.height,
                    radius: Math.random() * 1.5 + 0.5,
                    vx: (Math.random() - 0.5) * 0.3,
                    vy: (Math.random() - 0.5) * 0.3,
                    alpha: Math.random() * 0.5 + 0.1
                });
            }
        }

        function animateBg() {
            if (!bgCtx) return;
            
            bgCtx.fillStyle = '#0a0c10';
            bgCtx.fillRect(0, 0, bgCanvas.width, bgCanvas.height);
            
            // Draw gradient overlay
            const gradient = bgCtx.createRadialGradient(
                bgCanvas.width / 2, bgCanvas.height / 2, 0,
                bgCanvas.width / 2, bgCanvas.height / 2, bgCanvas.width * 0.7
            );
            gradient.addColorStop(0, 'rgba(0, 212, 170, 0.03)');
            gradient.addColorStop(0.5, 'rgba(14, 165, 233, 0.02)');
            gradient.addColorStop(1, 'rgba(10, 12, 16, 0)');
            bgCtx.fillStyle = gradient;
            bgCtx.fillRect(0, 0, bgCanvas.width, bgCanvas.height);
            
            // Draw particles
            bgParticles.forEach(p => {
                p.x += p.vx;
                p.y += p.vy;
                
                if (p.x < 0) p.x = bgCanvas.width;
                if (p.x > bgCanvas.width) p.x = 0;
                if (p.y < 0) p.y = bgCanvas.height;
                if (p.y > bgCanvas.height) p.y = 0;
                
                bgCtx.beginPath();
                bgCtx.arc(p.x, p.y, p.radius, 0, Math.PI * 2);
                bgCtx.fillStyle = `rgba(0, 212, 170, ${p.alpha})`;
                bgCtx.fill();
            });
            
            // Draw connections
            bgCtx.strokeStyle = 'rgba(0, 212, 170, 0.05)';
            bgCtx.lineWidth = 0.5;
            for (let i = 0; i < bgParticles.length; i++) {
                for (let j = i + 1; j < bgParticles.length; j++) {
                    const dx = bgParticles[i].x - bgParticles[j].x;
                    const dy = bgParticles[i].y - bgParticles[j].y;
                    const dist = Math.sqrt(dx * dx + dy * dy);
                    
                    if (dist < 120) {
                        bgCtx.beginPath();
                        bgCtx.moveTo(bgParticles[i].x, bgParticles[i].y);
                        bgCtx.lineTo(bgParticles[j].x, bgParticles[j].y);
                        bgCtx.stroke();
                    }
                }
            }
            
            requestAnimationFrame(animateBg);
        }

        // ==========================================
        // GRAPH CANVAS RENDERING
        // ==========================================
        const graphCanvas = document.getElementById('graph-canvas');
        const graphCtx = graphCanvas ? graphCanvas.getContext('2d') : null;
        const minimapCanvas = document.getElementById('minimap-canvas');
        const minimapCtx = minimapCanvas ? minimapCanvas.getContext('2d') : null;

        function initGraphCanvas() {
            if (!graphCanvas) return;
            const container = graphCanvas.parentElement;
            graphCanvas.width = container.clientWidth;
            graphCanvas.height = container.clientHeight;
            
            if (minimapCanvas) {
                minimapCanvas.width = 180;
                minimapCanvas.height = 120;
            }
        }

        function getNodeColor(term) {
            const category = AIKnowledgeBase.categories.find(c => c.id === term.category);
            return category ? category.color : '#00d4aa';
        }

        function drawGraph() {
            if (!graphCtx || !graphCanvas) return;
            
            graphCtx.clearRect(0, 0, graphCanvas.width, graphCanvas.height);
            
            // Apply transformations
            graphCtx.save();
            graphCtx.translate(state.graph.panX, state.graph.panY);
            graphCtx.scale(state.graph.zoom, state.graph.zoom);
            
            // Draw edges
            state.graph.edges.forEach(edge => {
                const sourceNode = state.graph.nodes.find(n => n.id === edge.source);
                const targetNode = state.graph.nodes.find(n => n.id === edge.target);
                
                if (sourceNode && targetNode) {
                    const isHighlighted = state.selectedNode && 
                        (state.selectedNode.id === sourceNode.id || state.selectedNode.id === targetNode.id);
                    
                    graphCtx.beginPath();
                    graphCtx.moveTo(sourceNode.x, sourceNode.y);
                    graphCtx.lineTo(targetNode.x, targetNode.y);
                    
                    if (isHighlighted) {
                        graphCtx.strokeStyle = 'rgba(0, 212, 170, 0.6)';
                        graphCtx.lineWidth = 2;
                    } else {
                        graphCtx.strokeStyle = 'rgba(48, 54, 61, 0.5)';
                        graphCtx.lineWidth = 1;
                    }
                    graphCtx.stroke();
                }
            });
            
            // Draw nodes
            state.graph.nodes.forEach(node => {
                const isSelected = state.selectedNode && state.selectedNode.id === node.id;
                const isHovered = state.hoveredNode && state.hoveredNode.id === node.id;
                const isRelated = state.selectedNode && node.term.related && 
                    node.term.related.includes(state.selectedNode.id);
                
                const color = getNodeColor(node.term);
                
                // Node glow
                if (isSelected || isHovered) {
                    const glowRadius = Math.max(1, node.radius + 15);
                    const glow = graphCtx.createRadialGradient(
                        node.x, node.y, 0,
                        node.x, node.y, glowRadius
                    );
                    glow.addColorStop(0, color + '40');
                    glow.addColorStop(1, 'rgba(0, 0, 0, 0)');
                    graphCtx.fillStyle = glow;
                    graphCtx.beginPath();
                    graphCtx.arc(node.x, node.y, glowRadius, 0, Math.PI * 2);
                    graphCtx.fill();
                }
                
                // Node circle
                graphCtx.beginPath();
                graphCtx.arc(node.x, node.y, Math.max(1, node.radius), 0, Math.PI * 2);
                
                if (isSelected) {
                    graphCtx.fillStyle = color;
                    graphCtx.strokeStyle = '#ffffff';
                    graphCtx.lineWidth = 3;
                } else if (isHovered || isRelated) {
                    graphCtx.fillStyle = color + 'cc';
                    graphCtx.strokeStyle = color;
                    graphCtx.lineWidth = 2;
                } else {
                    graphCtx.fillStyle = '#161b22';
                    graphCtx.strokeStyle = color + '80';
                    graphCtx.lineWidth = 2;
                }
                
                graphCtx.fill();
                graphCtx.stroke();
                
                // Node label
                graphCtx.font = `${isSelected || isHovered ? '600' : '500'} 12px Space Grotesk`;
                graphCtx.textAlign = 'center';
                graphCtx.textBaseline = 'middle';
                graphCtx.fillStyle = isSelected || isHovered ? '#ffffff' : '#e6edf3';
                
                // Truncate long names
                let label = node.term.name;
                if (label.length > 12) {
                    label = label.substring(0, 10) + '...';
                }
                graphCtx.fillText(label, node.x, node.y);
            });
            
            graphCtx.restore();
            
            // Draw minimap
            drawMinimap();
        }

        function drawMinimap() {
            if (!minimapCtx || !minimapCanvas || !graphCanvas) return;
            
            minimapCtx.fillStyle = '#0d1117';
            minimapCtx.fillRect(0, 0, minimapCanvas.width, minimapCanvas.height);
            
            const scale = Math.min(
                minimapCanvas.width / graphCanvas.width,
                minimapCanvas.height / graphCanvas.height
            ) * 0.9;
            
            const offsetX = (minimapCanvas.width - graphCanvas.width * scale) / 2;
            const offsetY = (minimapCanvas.height - graphCanvas.height * scale) / 2;
            
            // Draw edges
            minimapCtx.strokeStyle = 'rgba(48, 54, 61, 0.3)';
            minimapCtx.lineWidth = 0.5;
            state.graph.edges.forEach(edge => {
                const sourceNode = state.graph.nodes.find(n => n.id === edge.source);
                const targetNode = state.graph.nodes.find(n => n.id === edge.target);
                
                if (sourceNode && targetNode) {
                    minimapCtx.beginPath();
                    minimapCtx.moveTo(
                        offsetX + sourceNode.x * scale * state.graph.zoom,
                        offsetY + sourceNode.y * scale * state.graph.zoom
                    );
                    minimapCtx.lineTo(
                        offsetX + targetNode.x * scale * state.graph.zoom,
                        offsetY + targetNode.y * scale * state.graph.zoom
                    );
                    minimapCtx.stroke();
                }
            });
            
            // Draw nodes
            state.graph.nodes.forEach(node => {
                minimapCtx.beginPath();
                const x = offsetX + node.x * scale * state.graph.zoom;
                const y = offsetY + node.y * scale * state.graph.zoom;
                minimapCtx.arc(x, y, 2, 0, Math.PI * 2);
                minimapCtx.fillStyle = getNodeColor(node.term) + '80';
                minimapCtx.fill();
            });
            
            // Draw viewport indicator
            minimapCtx.strokeStyle = 'rgba(0, 212, 170, 0.8)';
            minimapCtx.lineWidth = 1;
            minimapCtx.strokeRect(
                offsetX - state.graph.panX * scale,
                offsetY - state.graph.panY * scale,
                graphCanvas.width * scale / state.graph.zoom,
                graphCanvas.height * scale / state.graph.zoom
            );
        }

        // ==========================================
        // UI RENDERING
        // ==========================================
        function renderCategories() {
            const categoryList = document.getElementById('category-list');
            if (!categoryList) return;
            
            categoryList.innerHTML = AIKnowledgeBase.categories.map(cat => {
                const termCount = AIKnowledgeBase.terms.filter(t => t.category === cat.id).length;
                return `
                    <div class="category-item" data-category="${cat.id}">
                        <div class="category-icon" style="background: ${cat.color}20; color: ${cat.color};">
                            <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <circle cx="12" cy="12" r="10"/>
                            </svg>
                        </div>
                        <div class="category-info">
                            <div class="category-name">${cat.name}</div>
                            <div class="category-count">${termCount} terms</div>
                        </div>
                        <svg class="category-arrow" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="m9 18 6-6-6-6"/>
                        </svg>
                    </div>
                `;
            }).join('');
            
            // Update stats
            document.getElementById('stat-categories').textContent = AIKnowledgeBase.categories.length;
            document.getElementById('stat-terms').textContent = AIKnowledgeBase.terms.length;
            document.getElementById('stat-connections').textContent = state.graph.edges.length;
            document.getElementById('total-count').textContent = `${AIKnowledgeBase.terms.length} terms`;
            
            // Calculate max depth (sified)
            document.getElementById('stat-depth').textContent = '3';
        }

        function renderTermsList() {
            const termsList = document.getElementById('terms-list');
            if (!termsList) return;
            
            const filteredTerms = state.selectedCategory 
                ? AIKnowledgeBase.terms.filter(t => t.category === state.selectedCategory)
                : AIKnowledgeBase.terms;
            
            const searchLower = state.searchQuery.toLowerCase();
            const searchedTerms = state.searchQuery
                ? filteredTerms.filter(t => 
                    t.name.toLowerCase().includes(searchLower) ||
                    t.shortDesc.toLowerCase().includes(searchLower) ||
                    (t.fullName && t.fullName.toLowerCase().includes(searchLower))
                )
                : filteredTerms;
            
            termsList.innerHTML = searchedTerms.map((term, index) => {
                const category = AIKnowledgeBase.categories.find(c => c.id === term.category);
                return `
                    <div class="term-card fade-in" style="animation-delay: ${index * 50}ms" data-term-id="${term.id}">
                        <div class="term-card-header">
                            <h3 class="term-card-title">${term.name}</h3>
                            <span class="term-card-category" style="color: ${category ? category.color : '#7d8590'}">${category ? category.name : 'General'}</span>
                        </div>
                        <p class="term-card-desc">${term.shortDesc}</p>
                        <div class="term-card-footer">
                            ${term.tags.slice(0, 3).map(tag => `<span class="tag">${tag}</span>`).join('')}
                        </div>
                    </div>
                `;
            }).join('');
        }

        function showNodePanel(term) {
            const panel = document.getElementById('node-panel');
            const category = AIKnowledgeBase.categories.find(c => c.id === term.category);
            
            document.getElementById('node-badge').style.borderColor = category ? category.color : '#00d4aa';
            document.getElementById('node-category').textContent = category ? category.name : 'General';
            document.getElementById('node-title').textContent = term.fullName || term.name;
            document.getElementById('node-subtitle').textContent = term.shortDesc;
            document.getElementById('node-description').textContent = term.definition;
            
            // Render related terms
            const relatedContainer = document.getElementById('related-terms');
            if (term.related && term.related.length > 0) {
                relatedContainer.innerHTML = term.related.map(relId => {
                    const relTerm = AIKnowledgeBase.terms.find(t => t.id === relId);
                    if (!relTerm) return '';
                    const relCat = AIKnowledgeBase.categories.find(c => c.id === relTerm.category);
                    return `
                        <div class="related-item" data-term-id="${relTerm.id}">
                            <div class="related-item-name">${relTerm.name}</div>
                            <div class="related-item-type">${relCat ? relCat.name : 'General'}</div>
                        </div>
                    `;
                }).join('');
            } else {
                relatedContainer.innerHTML = '<p style="color: var(--fg-muted);">No related terms defined</p>';
            }
            
            // Render code example
            const codeContainer = document.getElementById('code-example');
            if (term.codeExample) {
                codeContainer.innerHTML = `<code>${escapeHtml(term.codeExample)}</code>`;
            } else {
                codeContainer.innerHTML = '<code>// No code example available</code>';
            }
            
            // Render tags
            const tagList = document.getElementById('tag-list');
            tagList.innerHTML = term.tags.map(tag => `<span class="tag">${tag}</span>`).join('');
            
            panel.classList.add('open');
            state.selectedNode = term;
            drawGraph();
        }

        function closeNodePanel() {
            document.getElementById('node-panel').classList.remove('open');
            state.selectedNode = null;
            drawGraph();
        }

        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        function populateAddTermForm() {
            const select = document.querySelector('#add-term-form select[name="category"]');
            if (!select) return;
            
            select.innerHTML = AIKnowledgeBase.categories.map(cat => 
                `<option value="${cat.id}">${cat.name} - ${cat.description}</option>`
            ).join('');
        }

        // ==========================================
        // EVENT HANDLERS
        // ==========================================
        function setupEventListeners() {
            // Search
            const searchInput = document.getElementById('search-input');
            searchInput.addEventListener('input', (e) => {
                state.searchQuery = e.target.value;
                renderTermsList();
                if (state.currentView === 'graph') {
                    highlightSearchResults();
                }
            });
            
            // Keyboard shortcut for search
            document.addEventListener('keydown', (e) => {
                if (e.key === '/' && document.activeElement !== searchInput) {
                    e.preventDefault();
                    searchInput.focus();
                }
                if (e.key === 'Escape') {
                    closeNodePanel();
                    document.getElementById('add-term-modal').classList.add('hidden');
                    document.getElementById('add-term-modal').classList.remove('flex');
                }
            });
            
            // Sidebar toggle
            document.getElementById('sidebar-toggle').addEventListener('click', () => {
                const sidebar = document.getElementById('sidebar');
                const mainContent = document.getElementById('main-content');
                sidebar.classList.toggle('collapsed');
                mainContent.classList.toggle('expanded');
            });
            
            // Category selection
            document.getElementById('category-list').addEventListener('click', (e) => {
                const categoryItem = e.target.closest('.category-item');
                if (categoryItem) {
                    const categoryId = categoryItem.dataset.category;
                    
                    // Toggle selection
                    document.querySelectorAll('.category-item').forEach(item => item.classList.remove('active'));
                    
                    if (state.selectedCategory === categoryId) {
                        state.selectedCategory = null;
                    } else {
                        categoryItem.classList.add('active');
                        state.selectedCategory = categoryId;
                    }
                    
                    renderTermsList();
                    highlightCategoryInGraph();
                }
            });
            
            // View toggle
            document.querySelectorAll('.view-btn').forEach(btn => {
                btn.addEventListener('click', () => {
                    document.querySelectorAll('.view-btn').forEach(b => b.classList.remove('active'));
                    btn.classList.add('active');
                    
                    const view = btn.dataset.view;
                    state.currentView = view;
                    
                    document.getElementById('graph-view').classList.toggle('hidden', view !== 'graph');
                    document.getElementById('list-view').classList.toggle('active', view === 'list');
                    
                    if (view === 'list') {
                        renderTermsList();
                    }
                });
            });
            
            // Graph interactions
            setupGraphInteractions();
            
            // Term card clicks
            document.getElementById('terms-list').addEventListener('click', (e) => {
                const card = e.target.closest('.term-card');
                if (card) {
                    const termId = card.dataset.termId;
                    const term = AIKnowledgeBase.terms.find(t => t.id === termId);
                    if (term) showNodePanel(term);
                }
            });
            
            // Related term clicks
            document.getElementById('related-terms').addEventListener('click', (e) => {
                const item = e.target.closest('.related-item');
                if (item) {
                    const termId = item.dataset.termId;
                    const term = AIKnowledgeBase.terms.find(t => t.id === termId);
                    if (term) showNodePanel(term);
                }
            });
            
            // Close panel
            document.getElementById('close-panel').addEventListener('click', closeNodePanel);
            
            // Add term modal
            document.getElementById('add-term-btn').addEventListener('click', () => {
                const modal = document.getElementById('add-term-modal');
                modal.classList.remove('hidden');
                modal.classList.add('flex');
            });
            
            document.getElementById('cancel-add-term').addEventListener('click', () => {
                const modal = document.getElementById('add-term-modal');
                modal.classList.add('hidden');
                modal.classList.remove('flex');
            });
            
            document.getElementById('add-term-form').addEventListener('submit', (e) => {
                e.preventDefault();
                const formData = new FormData(e.target);
                
                const newTerm = {
                    id: formData.get('name').toLowerCase().replace(/\s+/g, '-'),
                    name: formData.get('name'),
                    category: formData.get('category'),
                    type: 'technique',
                    shortDesc: formData.get('shortDesc'),
                    definition: formData.get('definition'),
                    related: formData.get('related') ? formData.get('related').split(',').map(s => s.trim().toLowerCase().replace(/\s+/g, '-')) : [],
                    tags: formData.get('tags') ? formData.get('tags').split(',').map(s => s.trim()) : [],
                    codeExample: ''
                };
                
                // Add to knowledge base
                AIKnowledgeBase.terms.push(newTerm);
                
                // Add node to graph
                const category = AIKnowledgeBase.categories.find(c => c.id === newTerm.category);
                const categoryIndex = AIKnowledgeBase.categories.indexOf(category);
                const angle = (categoryIndex / AIKnowledgeBase.categories.length) * Math.PI * 2;
                
                state.graph.nodes.push({
                    id: newTerm.id,
                    x: window.innerWidth / 2 - 160 + Math.cos(angle) * 300,
                    y: window.innerHeight / 2 + Math.sin(angle) * 300,
                    vx: 0,
                    vy: 0,
                    radius: 25,
                    term: newTerm
                });
                
                // Add edges
                newTerm.related.forEach(relId => {
                    state.graph.edges.push({
                        source: newTerm.id,
                        target: relId,
                        strength: 0.5
                    });
                });
                
                // Update UI
                renderCategories();
                renderTermsList();
                drawGraph();
                
                // Close modal
                document.getElementById('add-term-modal').classList.add('hidden');
                document.getElementById('add-term-modal').classList.remove('flex');
                e.target.reset();
            });
            
            // Graph controls
            document.getElementById('zoom-in').addEventListener('click', () => {
                state.graph.zoom = Math.min(3, state.graph.zoom * 1.2);
                drawGraph();
            });
            
            document.getElementById('zoom-out').addEventListener('click', () => {
                state.graph.zoom = Math.max(0.3, state.graph.zoom / 1.2);
                drawGraph();
            });
            
            document.getElementById('reset-view').addEventListener('click', () => {
                state.graph.zoom = 1;
                state.graph.panX = 0;
                state.graph.panY = 0;
                drawGraph();
            });
            
            // Window resize
            window.addEventListener('resize', () => {
                initBgCanvas();
                initGraphCanvas();
                drawGraph();
            });
        }

        function setupGraphInteractions() {
            if (!graphCanvas) return;
            
            let lastMouseX = 0;
            let lastMouseY = 0;
            
            graphCanvas.addEventListener('mousedown', (e) => {
                state.isDragging = true;
                lastMouseX = e.clientX;
                lastMouseY = e.clientY;
                graphCanvas.style.cursor = 'grabbing';
            });
            
            graphCanvas.addEventListener('mousemove', (e) => {
                const rect = graphCanvas.getBoundingClientRect();
                const mouseX = (e.clientX - rect.left - state.graph.panX) / state.graph.zoom;
                const mouseY = (e.clientY - rect.top - state.graph.panY) / state.graph.zoom;
                
                if (state.isDragging) {
                    const dx = e.clientX - lastMouseX;
                    const dy = e.clientY - lastMouseY;
                    state.graph.panX += dx;
                    state.graph.panY += dy;
                    lastMouseX = e.clientX;
                    lastMouseY = e.clientY;
                    drawGraph();
                } else {
                    // Check for hover
                    let hoveredNode = null;
                    for (const node of state.graph.nodes) {
                        const dx = mouseX - node.x;
                        const dy = mouseY - node.y;
                        const dist = Math.sqrt(dx * dx + dy * dy);
                        if (dist < node.radius + 5) {
                            hoveredNode = node;
                            break;
                        }
                    }
                    
                    if (hoveredNode !== state.hoveredNode) {
                        state.hoveredNode = hoveredNode;
                        graphCanvas.style.cursor = hoveredNode ? 'pointer' : 'grab';
                        drawGraph();
                        
                        // Show tooltip
                        const tooltip = document.getElementById('tooltip');
                        if (hoveredNode) {
                            tooltip.innerHTML = `
                                <strong>${hoveredNode.term.name}</strong><br>
                                <span style="color: var(--fg-muted)">${hoveredNode.term.shortDesc}</span>
                            `;
                            tooltip.style.left = e.clientX + 15 + 'px';
                            tooltip.style.top = e.clientY + 15 + 'px';
                            tooltip.classList.add('visible');
                        } else {
                            tooltip.classList.remove('visible');
                        }
                    }
                }
            });
            
            graphCanvas.addEventListener('mouseup', () => {
                state.isDragging = false;
                graphCanvas.style.cursor = state.hoveredNode ? 'pointer' : 'grab';
            });
            
            graphCanvas.addEventListener('mouseleave', () => {
                state.isDragging = false;
                state.hoveredNode = null;
                document.getElementById('tooltip').classList.remove('visible');
                drawGraph();
            });
            
            graphCanvas.addEventListener('click', (e) => {
                if (state.hoveredNode) {
                    showNodePanel(state.hoveredNode.term);
                }
            });
            
            graphCanvas.addEventListener('wheel', (e) => {
                e.preventDefault();
                const zoomFactor = e.deltaY > 0 ? 0.9 : 1.1;
                state.graph.zoom = Math.min(3, Math.max(0.3, state.graph.zoom * zoomFactor));
                drawGraph();
            });
        }

        function highlightSearchResults() {
            if (!state.searchQuery) {
                drawGraph();
                return;
            }
            
            const query = state.searchQuery.toLowerCase();
            state.graph.nodes.forEach(node => {
                node.highlighted = 
                    node.term.name.toLowerCase().includes(query) ||
                    node.term.shortDesc.toLowerCase().includes(query);
            });
            
            drawGraph();
        }

        function highlightCategoryInGraph() {
            drawGraph();
        }

        // ==========================================
        // SIMPLE PHYSICS SIMULATION
        // ==========================================
        function simulatePhysics() {
            const alpha = 0.1;
            const centerX = graphCanvas ? graphCanvas.width / 2 : 400;
            const centerY = graphCanvas ? graphCanvas.height / 2 : 300;
            
            // Apply forces
            state.graph.nodes.forEach(node => {
                // Center gravity
                node.vx += (centerX - node.x) * 0.0001;
                node.vy += (centerY - node.y) * 0.0001;
                
                // Node repulsion
                state.graph.nodes.forEach(other => {
                    if (node.id === other.id) return;
                    const dx = node.x - other.x;
                    const dy = node.y - other.y;
                    const dist = Math.max(1, Math.sqrt(dx * dx + dy * dy));
                    const force = 500 / (dist * dist);
                    node.vx += (dx / dist) * force;
                    node.vy += (dy / dist) * force;
                });
            });
            
            // Edge attraction
            state.graph.edges.forEach(edge => {
                const source = state.graph.nodes.find(n => n.id === edge.source);
                const target = state.graph.nodes.find(n => n.id === edge.target);
                if (!source || !target) return;
                
                const dx = target.x - source.x;
                const dy = target.y - source.y;
                const dist = Math.sqrt(dx * dx + dy * dy);
                const force = (dist - 150) * 0.005;
                
                source.vx += (dx / dist) * force;
                source.vy += (dy / dist) * force;
                target.vx -= (dx / dist) * force;
                target.vy -= (dy / dist) * force;
            });
            
            // Apply velocity with damping
            state.graph.nodes.forEach(node => {
                node.x += node.vx * alpha;
                node.y += node.vy * alpha;
                node.vx *= 0.9;
                node.vy *= 0.9;
            });
            
            drawGraph();
            requestAnimationFrame(simulatePhysics);
        }

        // ==========================================
        // INITIALIZATION
        // ==========================================
        function init() {
            initBgCanvas();
            animateBg();
            
            initializeGraphNodes();
            initGraphCanvas();
            
            renderCategories();
            populateAddTermForm();
            
            setupEventListeners();
            
            // Start physics simulation
            simulatePhysics();
        }

        // Wait for DOM
        if (document.readyState === 'loading') {
            document.addEventListener('DOMContentLoaded', init);
        } else {
            init();
        }
    </script>
</body>
</html>
